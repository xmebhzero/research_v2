{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In cross-validation, also known as CV, the training data is split into five folds (any number will do, but five is standard). \n",
    "# The ML algorithm is fit on one fold at a time and tested on the remaining data.\n",
    "# The result is five different training and test sets that are all representative of the same data. \n",
    "# The mean of the scores is usually taken as the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv('./datasets/HousingData.csv')\n",
    "housing_df.head()\n",
    "\n",
    "# Drop null value\n",
    "housing_df = housing_df.dropna()\n",
    "\n",
    "# X for the predictor columns and y for the target column\n",
    "X = housing_df.iloc[:,:-1]\n",
    "y = housing_df.iloc[:, -1] # The target column is MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [3.26123843 4.42712448 5.66151114 8.09493087 5.24453989]\n",
      "Reg mean: 5.3378689628783516\n"
     ]
    }
   ],
   "source": [
    "def regression_model_cv(model, k=5):\n",
    "  # Since mean_squared_error is not an option for cross_val_score, we choose neg_mean_squared_error\n",
    "  # cross_val_score takes the highest value by default, and the highest negative mean squared error is 0\n",
    "  scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=k)\n",
    "  rmse = np.sqrt(-scores)\n",
    "  print('Reg rmse:', rmse)\n",
    "  print('Reg mean:', rmse.mean())\n",
    "\n",
    "regression_model_cv(LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [ 3.72504914  6.01655701 23.20863933]\n",
      "Reg mean: 10.983415161090788\n"
     ]
    }
   ],
   "source": [
    "# Let's try with 3 folds\n",
    "regression_model_cv(LinearRegression(), k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [3.23879491 3.97041949 5.58329663 3.92861033 9.88399671 3.91442679]\n",
      "Reg mean: 5.086590810801092\n"
     ]
    }
   ],
   "source": [
    "# Let's try with 6 folds\n",
    "regression_model_cv(LinearRegression(), k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization is an important concept in ML; it’s used to counteract overfitting.\n",
    "# In the world of big data, it’s easy to overfit data to the training set. \n",
    "# When this happens, the model will often perform badly on the test set, as indicated by mean_squared_error or some other error.\n",
    "\n",
    "# There are two main problems with fitting an ML model on all the data:\n",
    "# 1. There is no way to test the model on unseen data. ML models are powerful when they make\n",
    "# good predictions on new data. Models are trained on known results, but they perform in the\n",
    "# real world on data that has never been seen before. It’s not vital to see how well a model fits\n",
    "# known results (the training set), but it’s absolutely crucial to see how well it performs on unseen\n",
    "# data (the test set).\n",
    "# 2. The model may overfit the data. Models exist that may fit any set of data points perfectly.\n",
    "\n",
    "# There are many models and approaches to counteract overfitting. Let’s go over a couple of linear models now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [3.17202127 4.54972372 5.36604368 8.03715216 5.03988501]\n",
      "Reg mean: 5.23296516625177\n"
     ]
    }
   ],
   "source": [
    "# 1. Ridge = Ridge includes an L2 penalty term (L2 is based on Euclidean distance) that shrinks the linear coefficients\n",
    "#            based on their size. The coefficients are the weights—numbers that determine how influential\n",
    "#            each column is on the output. Larger weights carry greater penalties in Ridge.\n",
    "regression_model_cv(Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [3.52318747 5.70083491 7.82318757 6.9878025  3.97229348]\n",
      "Reg mean: 5.60146118538429\n"
     ]
    }
   ],
   "source": [
    "# 2. Lasso = Lasso adds a penalty equal to the absolute value of the magnitude of coefficients. This L1 regularization (L1 is taxicab distance)\n",
    "#            can eliminate some column influence, but it’s less widely used than Ridge on account of the\n",
    "#            L1 distance metric being less common than L2.\n",
    "regression_model_cv(Lasso())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_2nd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
